{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e0d6660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights…\n",
      "#0  loading model-00016-of-00016.safetensors …\n",
      "#1  loading model-00001-of-00016.safetensors …\n",
      "#2  loading model-00004-of-00016.safetensors …\n",
      "#3  loading model-00005-of-00016.safetensors …\n",
      "#4  loading model-00006-of-00016.safetensors …\n",
      "#5  loading model-00007-of-00016.safetensors …\n",
      "#6  loading model-00002-of-00016.safetensors …\n",
      "#7  loading model-00008-of-00016.safetensors …\n",
      "#8  loading model-00009-of-00016.safetensors …\n",
      "#9  loading model-00010-of-00016.safetensors …\n",
      "#10  loading model-00011-of-00016.safetensors …\n",
      "#11  loading model-00012-of-00016.safetensors …\n",
      "#12  loading model-00013-of-00016.safetensors …\n",
      "#13  loading model-00014-of-00016.safetensors …\n",
      "#14  loading model-00015-of-00016.safetensors …\n",
      "#15  loading model-00003-of-00016.safetensors …\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QwenMoe(\n",
       "  (model): _QwenBody(\n",
       "    (embed_tokens): Embedding(151936, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-47): 48 x DecoderLayer(\n",
       "        (self_attn): QwenAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "          (q_norm): RMSNorm()\n",
       "          (k_norm): RMSNorm()\n",
       "        )\n",
       "        (mlp): SparseMoe(\n",
       "          (gate): Linear(in_features=2048, out_features=128, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-127): 128 x ExpertMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (down_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# import the bits you need\n",
    "from qwen3_manual import QwenMoe, load_weights, generate\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# 1) load config & tokenizer\n",
    "model_dir = \"/home/zheng/LLMs/Qwen3-30B-A3B\"\n",
    "with open(os.path.join(model_dir, \"config.json\")) as f:\n",
    "    cfg = json.load(f)\n",
    "tokenizer = Tokenizer.from_file(os.path.join(model_dir, \"tokenizer.json\"))\n",
    "\n",
    "# 2) build the model\n",
    "model = QwenMoe(cfg).to(memory_format=torch.channels_last)\n",
    "#   (the script sets TARGET_DTYPE from its CLI; \n",
    "#    if you want float32, set torch.TARGET_DTYPE before loading)\n",
    "\n",
    "# 3) load the weights\n",
    "print(\"Loading weights…\")\n",
    "load_weights(model, model_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9f4e7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from notebook演示常用常用常用\n"
     ]
    }
   ],
   "source": [
    "# 4) prepare input IDs\n",
    "prompt = \"Hello from notebook\"\n",
    "input_ids = torch.tensor([tokenizer.encode(prompt).ids], dtype=torch.long)\n",
    "\n",
    "# 5) generate\n",
    "out_ids = generate(\n",
    "    model, input_ids,\n",
    "    max_new=4,\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    eos=tokenizer.token_to_id(\"<|im_end|>\")\n",
    ")\n",
    "\n",
    "# 6) decode & inspect\n",
    "generated = tokenizer.decode(out_ids[0].tolist())\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c56952a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Give me a short introduction to large language model.<|im_end|>\n",
      "<|im_start|>assistant\n",
      " hashtag webinar deadline webinar webinar deadline免费免费免费免费免费免费免费免费免费免费\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "# Prepare the model input\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switch between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "\n",
    "# Initialize the input ids for the generation\n",
    "input_ids = model_inputs['input_ids']\n",
    "\n",
    "# 5) generate\n",
    "out_ids = generate(\n",
    "    model, input_ids,\n",
    "    max_new=16,\n",
    "    temperature=0,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    eos=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# 6) decode & inspect\n",
    "generated = tokenizer.decode(out_ids[0].tolist())\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e36f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓  our model ready\n",
      "user\n",
      "Give me a short introduction to large language model.\n",
      "assistant\n",
      " hashtag webinar deadline webinar webinar deadline免费免费免费免费免费免费免费免费免费免费免费免费免费免费免费免费免费免费免费免费免费免费免费订阅免费订阅\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Qwen3MoeRotaryEmbedding.forward() missing 1 required positional argument: 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 137\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m L \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cfg[\u001b[33m\"\u001b[39m\u001b[33mnum_hidden_layers\u001b[39m\u001b[33m\"\u001b[39m]):\n\u001b[32m    136\u001b[39m     ours = ours_hidden(ids, L)\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     ref  = \u001b[43mhf_one_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch.allclose(ours, ref, atol=\u001b[32m1e-2\u001b[39m, rtol=\u001b[32m1e-2\u001b[39m):\n\u001b[32m    139\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mL\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m matches\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 120\u001b[39m, in \u001b[36mhf_one_layer\u001b[39m\u001b[34m(input_ids, layer_idx)\u001b[39m\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mseq_len\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sig.parameters:\n\u001b[32m    119\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mseq_len\u001b[39m\u001b[33m\"\u001b[39m] = T\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     cos, sin = \u001b[43mrope\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m cos = cos.unsqueeze(\u001b[32m0\u001b[39m).unsqueeze(\u001b[32m2\u001b[39m)\n\u001b[32m    123\u001b[39m sin = sin.unsqueeze(\u001b[32m0\u001b[39m).unsqueeze(\u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen3-cpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen3-cpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen3-cpu/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: Qwen3MoeRotaryEmbedding.forward() missing 1 required positional argument: 'x'"
     ]
    }
   ],
   "source": [
    "# ───────────────────────────────────────────────────────────────\n",
    "# 0.  Imports & paths\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "import os, json, gc, math, torch, safetensors.torch as st\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "from qwen3_manual import QwenMoe, load_weights        # ← your script\n",
    "\n",
    "\n",
    "model_dir = \"/home/zheng/LLMs/Qwen3-30B-A3B\"\n",
    "tok = Tokenizer.from_file(os.path.join(model_dir, \"tokenizer.json\"))\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# 1.  Build & load *our* model (once)\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "with open(os.path.join(model_dir, \"config.json\")) as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "our_model = model\n",
    "\n",
    "print(\"✓  our model ready\")\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# 2.  Safer generate()  (handles temperature = 0)\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "@torch.no_grad()\n",
    "def generate(model, input_ids, max_new=16, temperature=0.7,\n",
    "             top_p=0.95, top_k=50, eos=None):\n",
    "    for _ in range(max_new):\n",
    "        logits = model(input_ids)[:, -1, :]\n",
    "        if temperature == 0:                       # greedy\n",
    "            next_token = logits.argmax(-1, keepdim=True)\n",
    "        else:\n",
    "            logits = logits / temperature\n",
    "            top_k = min(top_k, logits.size(-1))\n",
    "            topk_vals, topk_idx = torch.topk(logits, top_k, dim=-1)\n",
    "            probs = torch.softmax(topk_vals, -1)\n",
    "            next_token = topk_idx.gather(-1, torch.multinomial(probs, 1))\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "        if eos is not None and next_token.item() == eos:\n",
    "            break\n",
    "    return input_ids\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# 3.  Quick sanity: greedy generation\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "prompt = \"<|im_start|>user\\nGive me a short introduction to large language model.<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "ids = torch.tensor([tok.encode(prompt).ids])\n",
    "print(tok.decode(generate(our_model, ids, max_new=32, temperature=0)[0].tolist()))\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# 4.  Helper: run *our* network up to layer N\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "def ours_hidden(input_ids, upto):\n",
    "    h = our_model.model.embed_tokens(input_ids)\n",
    "    for i in range(upto + 1):\n",
    "        h = our_model.model.layers[i](h, our_model.model.rope)\n",
    "    return our_model.model.norm(h)\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# 5.  Helper: run HF **single layer** without loading the whole net\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# ── fixed helper: run a single HF layer without loading the whole model ──\n",
    "# ── helper: run a single HF layer without loading full model ────────────\n",
    "# ── helper: run a single HF layer, robust to 4.48 → 4.51 APIs ────────────────\n",
    "# ── run a single HF layer without loading the whole model ─────────────\n",
    "def hf_one_layer(input_ids, layer_idx):\n",
    "    import gc, json, torch, safetensors.torch as st\n",
    "    from transformers import AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "    # 1) config trimmed to 1 decoder layer\n",
    "    cfg = AutoConfig.from_pretrained(model_dir)\n",
    "    cfg.num_hidden_layers = 1\n",
    "    hf = AutoModelForCausalLM.from_config(cfg, torch_dtype=torch.bfloat16).eval()\n",
    "\n",
    "    # 2) load only needed weights\n",
    "    wanted = [\n",
    "        \"model.embed_tokens\",\n",
    "        f\"model.layers.{layer_idx}\",\n",
    "        \"model.norm\",\n",
    "        \"model.rotary_emb\",\n",
    "    ]\n",
    "    fmap = json.load(open(os.path.join(model_dir, \"model.safetensors.index.json\")))[\"weight_map\"]\n",
    "    names = [n for n in fmap if any(n.startswith(w) for w in wanted)]\n",
    "    shard_to_names = {}\n",
    "    for n in names:\n",
    "        shard_to_names.setdefault(fmap[n], []).append(n)\n",
    "\n",
    "    sd = hf.state_dict()\n",
    "    for shard, keys in shard_to_names.items():\n",
    "        tensors = st.load_file(os.path.join(model_dir, shard), device=\"cpu\")\n",
    "        for k in keys:\n",
    "            sd[k].copy_(tensors[k].to(sd[k].dtype))\n",
    "        del tensors\n",
    "        gc.collect()\n",
    "\n",
    "    # 3) forward through the single layer (let HF handle RoPE internally)\n",
    "    with torch.no_grad():\n",
    "        h = hf.model.embed_tokens(input_ids)          # [B,T,D]\n",
    "        pos_ids = torch.arange(h.size(1), device=h.device).unsqueeze(0)\n",
    "        h = hf.model.layers[0](h, position_ids=pos_ids)   # no attention_mask\n",
    "        h = hf.model.norm(h)\n",
    "\n",
    "    del hf, sd\n",
    "    gc.collect()\n",
    "    return h\n",
    "\n",
    "\n",
    "    del hf, state; gc.collect()\n",
    "    return h\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# 6.  Compare layers one-by-one (always <4 GB at a time)\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "ids = torch.tensor([tok.encode(\"Hello\").ids])\n",
    "for L in range(cfg[\"num_hidden_layers\"]):\n",
    "    ours = ours_hidden(ids, L)\n",
    "    ref  = hf_one_layer(ids, L)\n",
    "    if torch.allclose(ours, ref, atol=1e-2, rtol=1e-2):\n",
    "        print(f\"✓ layer {L} matches\")\n",
    "    else:\n",
    "        print(f\"❌ divergence at layer {L}\")\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen3-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
